import os
from typing import Optional, List, Dict, Any
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
import joblib


# -------------------- –í–ù–£–¢–†–Ü–®–ù–Ü –ü–Ü–î–§–£–ù–ö–¶–Ü–á --------------------

def _drop_unused_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    –í–∏–¥–∞–ª—è—î —Ç–µ—Ö–Ω—ñ—á–Ω—ñ –∞–±–æ —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ–π–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏, –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω—ñ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è.

    Args:
        df (pd.DataFrame): –í—Ö—ñ–¥–Ω–∏–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º.

    Returns:
        pd.DataFrame: –î–∞—Ç–∞—Ñ—Ä–µ–π–º –±–µ–∑ –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫.
    """
    return df.drop(['id', 'CustomerId', 'Surname'], axis=1, errors='ignore')


def _add_product_group(df: pd.DataFrame) -> pd.DataFrame:
    """
    –°—Ç–≤–æ—Ä—é—î –Ω–æ–≤—É –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω—É –æ–∑–Ω–∞–∫—É 'ProductGroup' –Ω–∞ –æ—Å–Ω–æ–≤—ñ 'NumOfProducts'.

    Args:
        df (pd.DataFrame): –í—Ö—ñ–¥–Ω–∏–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º.

    Returns:
        pd.DataFrame: –ö–æ–ø—ñ—è –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—É –∑ –¥–æ–¥–∞–Ω–æ—é –∫–æ–ª–æ–Ω–∫–æ—é 'ProductGroup'.
    """
    def simplify_products(x: int) -> str:
        if x == 1:
            return '1'
        elif x == 2:
            return '2'
        else:
            return '3'

    df['ProductGroup'] = df['NumOfProducts'].apply(simplify_products)
    return df


def _scale_numeric_features(
    train_df: pd.DataFrame, val_df: pd.DataFrame, numeric_cols: List[str]
) -> tuple[pd.DataFrame, pd.DataFrame, MinMaxScaler]:
    """
    –ú–∞—Å—à—Ç–∞–±—É—î —á–∏—Å–ª–æ–≤—ñ –æ–∑–Ω–∞–∫–∏ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é MinMaxScaler.

    Args:
        train_df (pd.DataFrame): –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏–π –Ω–∞–±—ñ—Ä.
        val_df (pd.DataFrame): –í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∏–π –∞–±–æ —Ç–µ—Å—Ç–æ–≤–∏–π –Ω–∞–±—ñ—Ä.
        numeric_cols (List[str]): –°–ø–∏—Å–æ–∫ —á–∏—Å–ª–æ–≤–∏—Ö –∫–æ–ª–æ–Ω–æ–∫.

    Returns:
        tuple[pd.DataFrame, pd.DataFrame, MinMaxScaler]: 
        –ú–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω—ñ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∏ —Ç–∞ –æ–±'—î–∫—Ç scaler.
    """
    scaler = MinMaxScaler()
    scaler.fit(train_df[numeric_cols])

    train_scaled = train_df.copy()
    val_scaled = val_df.copy()

    train_scaled[numeric_cols] = scaler.transform(train_df[numeric_cols])
    val_scaled[numeric_cols] = scaler.transform(val_df[numeric_cols])

    return train_scaled, val_scaled, scaler


def _encode_categorical_features(
    train_df: pd.DataFrame, val_df: pd.DataFrame, categorical_cols: List[str]
) -> tuple[pd.DataFrame, pd.DataFrame, OneHotEncoder, List[str]]:
    """
    –í–∏–∫–æ–Ω—É—î One-Hot encoding –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫.

    Args:
        train_df (pd.DataFrame): –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏–π –Ω–∞–±—ñ—Ä.
        val_df (pd.DataFrame): –í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∏–π –Ω–∞–±—ñ—Ä.
        categorical_cols (List[str]): –°–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫.

    Returns:
        tuple[pd.DataFrame, pd.DataFrame, OneHotEncoder, List[str]]:
        –ó–∞–∫–æ–¥–æ–≤–∞–Ω—ñ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∏, encoder —ñ —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤ –Ω–æ–≤–∏—Ö –∫–æ–ª–æ–Ω–æ–∫.
    """
    encoder = OneHotEncoder(drop='first', sparse_output=False)
    encoder.fit(train_df[categorical_cols])

    encoded_cols = encoder.get_feature_names_out(categorical_cols)
    train_encoded = pd.DataFrame(
        encoder.transform(train_df[categorical_cols]),
        columns=encoded_cols, index=train_df.index
    )
    val_encoded = pd.DataFrame(
        encoder.transform(val_df[categorical_cols]),
        columns=encoded_cols, index=val_df.index
    )
    return train_encoded, val_encoded, encoder, list(encoded_cols)


# -------------------- –ü–£–ë–õ–Ü–ß–ù–Ü –§–£–ù–ö–¶–Ü–á --------------------

def preprocess_data(
    raw_df: pd.DataFrame,
    save_dir: str = "models",
    scaler_numeric: bool = True,
    test_size: float = 0.1
) -> Dict[str, Any]:
    """
    –ü–æ–≤–Ω–∞ –ø–æ–ø–µ—Ä–µ–¥–Ω—è –æ–±—Ä–æ–±–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è –∑–∞–¥–∞—á—ñ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –≤—ñ–¥—Ç–æ–∫—É –∫–ª—ñ—î–Ω—Ç—ñ–≤ –±–∞–Ω–∫—É.

    Args:
        raw_df (pd.DataFrame): –°–∏—Ä–∏–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º.
        save_dir (str): –¢–µ–∫–∞ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å–æ—Ä—ñ–≤.
        scaler_numeric (bool): –ß–∏ –º–∞—Å—à—Ç–∞–±—É–≤–∞—Ç–∏ —á–∏—Å–ª–æ–≤—ñ –æ–∑–Ω–∞–∫–∏ (–¥–ª—è –¥–µ—Ä–µ–≤ False).
        test_size (float): –ß–∞—Å—Ç–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä—É.

    Returns:
        dict[str, Any]: 
            –°–ª–æ–≤–Ω–∏–∫ —ñ–∑ train/val/test –Ω–∞–±–æ—Ä–∞–º–∏, –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å–æ—Ä–∞–º–∏ —Ç–∞ —Å–ø–∏—Å–∫–æ–º –æ–∑–Ω–∞–∫:
            {
                'train_X', 'train_y', 'val_X', 'val_y', 
                'test_X', 'test_y', 'input_cols', 'scaler', 'encoder'
            }
    """
    os.makedirs(save_dir, exist_ok=True)
    df = _drop_unused_columns(raw_df)
    target_col = 'Exited'

    # train/val/test split
    train_val_df, test_df = train_test_split(df, test_size=test_size, random_state=42, stratify=df[target_col])
    train_df, val_df = train_test_split(train_val_df, test_size=0.2, random_state=42, stratify=train_val_df[target_col])

    numeric_cols = ['Age', 'NumOfProducts', 'IsActiveMember', 'Balance']
    categorical_cols = ['Geography', 'Gender', 'ProductGroup']

    for subset in (train_df, val_df, test_df):
        _add_product_group(subset)

    scaler: Optional[MinMaxScaler] = None
    if scaler_numeric:
        train_df, val_df, scaler = _scale_numeric_features(train_df, val_df, numeric_cols)
        _, test_df, _ = _scale_numeric_features(train_df, test_df, numeric_cols)
        print("üìè –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è —á–∏—Å–ª–æ–≤–∏—Ö –æ–∑–Ω–∞–∫ —É–≤—ñ–º–∫–Ω–µ–Ω–æ.")
    else:
        print("üö´ –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è —á–∏—Å–ª–æ–≤–∏—Ö –æ–∑–Ω–∞–∫ –≤–∏–º–∫–Ω–µ–Ω–æ.")

    train_encoded, val_encoded, encoder, encoded_cols = _encode_categorical_features(train_df, val_df, categorical_cols)
    test_encoded = pd.DataFrame(
        encoder.transform(test_df[categorical_cols]),
        columns=encoded_cols, index=test_df.index
    )

    X_train = pd.concat([train_df.drop(columns=categorical_cols), train_encoded], axis=1)
    X_val = pd.concat([val_df.drop(columns=categorical_cols), val_encoded], axis=1)
    X_test = pd.concat([test_df.drop(columns=categorical_cols), test_encoded], axis=1)

    y_train, y_val, y_test = train_df[target_col], val_df[target_col], test_df[target_col]
    input_cols = X_train.columns.tolist()

    # save preprocessors
    if scaler is not None:
        joblib.dump(scaler, os.path.join(save_dir, "scaler.joblib"))
    joblib.dump(encoder, os.path.join(save_dir, "encoder.joblib"))
    print(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ encoder (—ñ scaler, —è–∫—â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–≤—Å—è) —É '{save_dir}'")

    return {
        'train_X': X_train,
        'train_y': y_train,
        'val_X': X_val,
        'val_y': y_val,
        'test_X': X_test,
        'test_y': y_test,
        'input_cols': input_cols,
        'scaler': scaler,
        'encoder': encoder
    }


def load_preprocessors(save_dir: str = "models") -> Dict[str, Optional[object]]:
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –∑–±–µ—Ä–µ–∂–µ–Ω—ñ scaler —Ç–∞ encoder.

    Args:
        save_dir (str): –¢–µ–∫–∞, –¥–µ –∑–±–µ—Ä–µ–∂–µ–Ω—ñ joblib-—Ñ–∞–π–ª–∏.

    Returns:
        dict[str, Optional[object]]: {'scaler': scaler –∞–±–æ None, 'encoder': encoder –∞–±–æ None}
    """
    scaler_path = os.path.join(save_dir, "scaler.joblib")
    encoder_path = os.path.join(save_dir, "encoder.joblib")

    scaler = encoder = None

    if os.path.exists(encoder_path):
        encoder = joblib.load(encoder_path)
        print("‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ encoder")
    else:
        print(f"‚ö†Ô∏è  –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª: {encoder_path}")

    if os.path.exists(scaler_path):
        scaler = joblib.load(scaler_path)
        print("‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ scaler")
    else:
        print(f"‚ÑπÔ∏è  –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –Ω–µ –∑–∞—Å—Ç–æ—Å–æ–≤—É–≤–∞–ª–æ—Å—å ({scaler_path} –≤—ñ–¥—Å—É—Ç–Ω—ñ–π)")

    return {'scaler': scaler, 'encoder': encoder}


def transform_new_data(
    new_df: pd.DataFrame, 
    preprocessors: Dict[str, Optional[object]]
) -> pd.DataFrame:
    """
    –û–±—Ä–æ–±–ª—è—î –Ω–æ–≤—ñ –¥–∞–Ω—ñ —Ç–∏–º —Å–∞–º–∏–º —Å–ø–æ—Å–æ–±–æ–º, —â–æ –π —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ.

    Args:
        new_df (pd.DataFrame): –ù–æ–≤—ñ —Å–∏—Ä—ñ –¥–∞–Ω—ñ (–±–µ–∑ 'Exited').
        preprocessors (dict): –°–ª–æ–≤–Ω–∏–∫ —ñ–∑ 'scaler' —ñ 'encoder'.

    Returns:
        pd.DataFrame: –û–±—Ä–æ–±–ª–µ–Ω–∏–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º, –≥–æ—Ç–æ–≤–∏–π –¥–æ –º–æ–¥–µ–ª—ñ.
    """
    df = new_df.copy()
    df = _drop_unused_columns(df)
    df = _add_product_group(df)

    numeric_cols = ['Age', 'NumOfProducts', 'IsActiveMember', 'Balance']
    categorical_cols = ['Geography', 'Gender', 'ProductGroup']

    scaler = preprocessors.get('scaler')
    encoder = preprocessors.get('encoder')

    if scaler:
        df[numeric_cols] = scaler.transform(df[numeric_cols])

    encoded_cols = encoder.get_feature_names_out(categorical_cols)
    encoded_array = encoder.transform(df[categorical_cols])
    encoded_df = pd.DataFrame(encoded_array, columns=encoded_cols, index=df.index)

    df_final = pd.concat([df.drop(columns=categorical_cols), encoded_df], axis=1)
    return df_final

def evaluate_model(pipeline, X, dataset_name='Dataset'):
    # 1. –ü—Ä–æ–≥–Ω–æ–∑ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π
    y_proba = pipeline.predict_proba(X)[:, 1]

    # 2. –ü—Ä–æ–≥–Ω–æ–∑ –∫–ª–∞—Å—ñ–≤ –ø—Ä–∏ –ø–æ—Ä–æ–∑—ñ 0.5
    y_pred = (y_proba >= 0.5).astype(int)

    # 3. Confusion Matrix
    cm = confusion_matrix(X['Exited'], y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot()
    plt.title(f'Confusion Matrix: {dataset_name}')
    plt.show()

    # 4. ROC Curve
    fpr, tpr, _ = roc_curve(X['Exited'], y_proba)
    RocCurveDisplay(fpr=fpr, tpr=tpr).plot()
    plt.title(f'ROC Curve: {dataset_name}')
    plt.show()

    # 5. –ú–µ—Ç—Ä–∏–∫–∏
    auc = roc_auc_score(X['Exited'], y_proba)
    f1 = f1_score(X['Exited'], y_pred)

    print(f"üìä {dataset_name} ‚Äî AUROC: {auc:.3f}, F1 Score (threshold=0.5): {f1:.3f}")
# -------------------- –¢–ï–°–¢–û–í–ò–ô –ó–ê–ü–£–°–ö --------------------
if __name__ == "__main__":
    csv_path = "train.csv"

    if not os.path.exists(csv_path):
        print("‚ö†Ô∏è  –§–∞–π–ª train.csv –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
    else:
        raw_df = pd.read_csv(csv_path)
        data = preprocess_data(raw_df, scaler_numeric=True)
        print("‚úÖ –û–±—Ä–æ–±–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. Train shape:", data['train_X'].shape)

        new_df = raw_df.sample(3, random_state=1).drop(columns=['Exited'])
        transformed = transform_new_data(new_df, {'scaler': data['scaler'], 'encoder': data['encoder']})
        print("üîÅ –ü—Ä–∏–∫–ª–∞–¥ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö:")
        print(transformed.head())
