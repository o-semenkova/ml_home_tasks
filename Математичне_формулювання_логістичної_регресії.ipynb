{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/o-semenkova/ml_home_tasks/blob/main/%D0%9C%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%BD%D0%B5_%D1%84%D0%BE%D1%80%D0%BC%D1%83%D0%BB%D1%8E%D0%B2%D0%B0%D0%BD%D0%BD%D1%8F_%D0%BB%D0%BE%D0%B3%D1%96%D1%81%D1%82%D0%B8%D1%87%D0%BD%D0%BE%D1%97_%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%96%D1%97.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Функція втрат для логістичної регресії\n",
        "\n",
        "У логістичній регресії функція втрат (також відома як логістична функція втрат або бінарна крос-ентропія) визначається як:\n",
        "\n",
        "$$\n",
        "L(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right]\n",
        "$$\n",
        "\n",
        "де:\n",
        "- $ m $ — кількість навчальних прикладів,\n",
        "- $ h_\\theta(x) $ — це гіпотеза моделі (логістична функція), яка визначається як:\n",
        "\n",
        "$$\n",
        "h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^T x}}\n",
        "$$\n",
        "\n",
        "### Похідні функції втрат для градієнтного спуску\n",
        "\n",
        "Для оптимізації параметрів $\\theta$ використовується градієнтний спуск. Необхідно знайти часткові похідні функції втрат $L(\\theta)$ за кожним параметром $\\theta_j$.\n",
        "\n",
        "#### Виведення похідних\n",
        "\n",
        "1. **Функція втрат**:\n",
        "\n",
        "$$\n",
        "L(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right]\n",
        "$$\n",
        "\n",
        "2. **Похідна по $\\theta_j$**:\n",
        "\n",
        "Розглянемо похідну функції втрат по параметру $\\theta_j$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L(\\theta)}{\\partial \\theta_j} = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\frac{\\partial}{\\partial \\theta_j} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\frac{\\partial}{\\partial \\theta_j} \\log(1 - h_\\theta(x^{(i)})) \\right]\n",
        "$$\n",
        "\n",
        "3. **Похідна логістичної функції**:\n",
        "\n",
        "Знайдемо часткову похідну логістичної функції:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial \\theta_j} h_\\theta(x^{(i)}) = h_\\theta(x^{(i)}) (1 - h_\\theta(x^{(i)})) x_j^{(i)}\n",
        "$$\n",
        "\n",
        "4. **Похідна логарифмів**:\n",
        "\n",
        "Похідна логарифма функції:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial \\theta_j} \\log(h_\\theta(x^{(i)})) = \\frac{1}{h_\\theta(x^{(i)})} \\cdot \\frac{\\partial h_\\theta(x^{(i)})}{\\partial \\theta_j} = \\frac{1}{h_\\theta(x^{(i)})} \\cdot h_\\theta(x^{(i)}) (1 - h_\\theta(x^{(i)})) x_j^{(i)} = (1 - h_\\theta(x^{(i)})) x_j^{(i)}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial \\theta_j} \\log(1 - h_\\theta(x^{(i)})) = \\frac{-1}{1 - h_\\theta(x^{(i)})} \\cdot \\frac{\\partial (1 - h_\\theta(x^{(i)}))}{\\partial \\theta_j} = \\frac{-1}{1 - h_\\theta(x^{(i)})} \\cdot (-h_\\theta(x^{(i)}) (1 - h_\\theta(x^{(i)})) x_j^{(i)}) = -h_\\theta(x^{(i)}) x_j^{(i)}\n",
        "$$\n",
        "\n",
        "5. **Об'єднання похідних**:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L(\\theta)}{\\partial \\theta_j} = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} (1 - h_\\theta(x^{(i)})) x_j^{(i)} + (1 - y^{(i)}) (-h_\\theta(x^{(i)})) x_j^{(i)} \\right]\n",
        "$$\n",
        "\n",
        "$$\n",
        "= -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} (1 - h_\\theta(x^{(i)})) x_j^{(i)} - (1 - y^{(i)}) h_\\theta(x^{(i)}) x_j^{(i)} \\right]\n",
        "$$\n",
        "\n",
        "$$\n",
        "= -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} x_j^{(i)} - y^{(i)} h_\\theta(x^{(i)}) x_j^{(i)} - h_\\theta(x^{(i)}) x_j^{(i)} + y^{(i)} h_\\theta(x^{(i)}) x_j^{(i)} \\right]\n",
        "$$\n",
        "\n",
        "$$\n",
        "= -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} x_j^{(i)} - h_\\theta(x^{(i)}) x_j^{(i)} \\right]\n",
        "$$\n",
        "\n",
        "$$\n",
        "= \\frac{1}{m} \\sum_{i=1}^{m} \\left[ (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\right]\n",
        "$$\n",
        "\n",
        "Отже, часткова похідна функції втрат по параметру $\\theta_j$ для логістичної регресії виглядає так:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\left[ (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\right]\n",
        "$$\n",
        "\n",
        "Ця похідна використовується в градієнтному спуску для оновлення параметрів моделі:\n",
        "\n",
        "$$\n",
        "\\theta_j := \\theta_j - \\alpha \\frac{\\partial L(\\theta)}{\\partial \\theta_j}\n",
        "$$\n",
        "\n",
        "де $\\alpha$ - це швидкість навчання (learning rate)."
      ],
      "metadata": {
        "id": "47akk6cjtF4y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mULFuu1tBRG"
      },
      "outputs": [],
      "source": []
    }
  ]
}